{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation and Saving Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 20:55:30.137775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocess_common import *\n",
    "\n",
    "from record_save_load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the HyperParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix some hyperparmeters like IMG_SIZE at a per model level, since each model excels at certain image sizes [1](https://link.springer.com/chapter/10.1007/978-3-030-86340-1_11). \n",
    "\n",
    "We also fix batch size and other parameters due to memory and compute constraints as well. We fix the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"archive/\"\n",
    "\n",
    "AUTO = tf_data.AUTOTUNE # Parallelize data loading\n",
    "#Hyperparameters\n",
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SIZE = 0.2\n",
    "RESIZE_SIZE = (512,512)\n",
    "\n",
    "SEED = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV's \n",
    "We load the csv from the original dataset here for further processing, [the kaggle site](https://www.kaggle.com/datasets/alessandrasala79/ai-vs-human-generated-dataset/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['Human', 'AI']\n",
    "train_df = pd.read_csv('./archive/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('./archive/test.csv')\n",
    "\n",
    "train_paths = train_df[\"file_name\"].array\n",
    "train_labels = train_df[\"label\"].array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Training and Validation Data\n",
    "\n",
    "We then split the training and validation data based off the class labels to ensure balanced class in the training and validation datasets.\n",
    "\n",
    "Once we split up the data, we use Tensorflows Data pipeline in order to apply our data augmentation`(ie. Flipping, rotating, color jitter)`, and resizing in a parallelized manner. We also set the seed to ensure reproducibility. We do not apply CutMix like in our draft and proposal because we found that since the test set included data generated by many models, CutMix could help learn more localizable features but it did not help learn the many other features prominent in the test set.\n",
    "\n",
    "### Explaining the code\n",
    "\n",
    "We apply `resize_augment_image` defined in preprocess_common.py which applies the `resizing, crops, flips, gaussian blur, and color jitter` to enhance model robustness as the test dataset is vastly different from the training set. We crop each image to a certain size for each model as each model excels at a certain input size [1](https://link.springer.com/chapter/10.1007/978-3-030-86340-1_11). \n",
    "\n",
    "We found that the models we are using like EfficientNet and ResNet have their own built in preprocessing function for scaling(ie. [0,1] or [-1,1] instead of [0,255]) and normalizing data so we refrain from applying it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 20:55:38.841951: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.697231: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.697285: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.698470: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.698560: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.698580: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.699065: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.699105: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.699111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-04-03 20:55:40.699153: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-03 20:55:40.699166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20716 MB memory:  -> device: 0, name: AMD Radeon RX 7900 XTX, pci bus id: 0000:00:00.3\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_labels.numpy() if isinstance(train_labels, tf.Tensor) else train_labels\n",
    "\n",
    "# Split the training data into training and validation sets balanced by label\n",
    "\n",
    "(train_paths, _, train_labels, _)= train_test_split(train_paths, train_labels, test_size=0.6, stratify=train_labels, random_state=SEED*2)\n",
    "\n",
    "\n",
    "(train_paths, val_paths, \n",
    " train_labels, val_labels) = train_test_split(train_paths, \n",
    "                 train_labels, \n",
    "                 test_size=VALIDATION_SIZE, \n",
    "                 stratify=train_labels,\n",
    "                 random_state=SEED)\n",
    " \n",
    "train_labels = keras.ops.one_hot(train_labels,2)\n",
    "val_labels = keras.ops.one_hot(val_labels,2)\n",
    "\n",
    "def create_datasets(train_paths, train_labels, val_paths, val_labels, image_size):\n",
    "    \"\"\" Creates the training and validation datasets for a certain image size.\n",
    "    \n",
    "        Args:\n",
    "            train_paths (list): list of paths to training images\n",
    "            train_labels (list): list of labels for training images\n",
    "            val_paths (list): list of paths to validation images\n",
    "            val_labels (list): list of labels for validation images\n",
    "            image_size (tuple): size to crop the images to\n",
    "        Returns:\n",
    "            (tuple): image tensor and label\n",
    "    \"\"\"\n",
    "    preprocess = Preprocess(image_size, image_size)\n",
    "    # Shuffles and batches the datasets\n",
    "    train_ds_one = (\n",
    "        tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "        .shuffle(BUFFER_SIZE, seed=SEED * 3)\n",
    "        .map(lambda filename, label: (preprocess.resize_augment_image(PATH+filename, augment=False,c_jitter=False),label), num_parallel_calls=AUTO, deterministic=True)\n",
    "        .batch(BATCH_SIZE, num_parallel_calls=AUTO, deterministic=True)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    # train_ds_two = (\n",
    "    #     tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    #     .shuffle(BUFFER_SIZE, seed=SEED * 2) \n",
    "    #     .map(lambda filename, label: (preprocess.resize_augment_image(PATH+filename, augment=True, c_jitter=True),label), num_parallel_calls=AUTO, deterministic=True)\n",
    "    # )\n",
    "    # Combine the two datasets for CutMix\n",
    "    # train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))\n",
    "    val_ds = (\n",
    "        tf_data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "        .map(lambda filename, label: (preprocess.resize_augment_image(PATH+filename),label), num_parallel_calls=AUTO, deterministic=True)\n",
    "        .batch(BATCH_SIZE, num_parallel_calls=AUTO, deterministic=True)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    return train_ds_one, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dataset(img_size, ram_budget):\n",
    "    \"\"\" Creates the training and validation datasets.\n",
    "    \n",
    "    Args:\n",
    "        img_size (tuple): size to crop the images to\n",
    "        ram_budget (int): RAM budget for autotuning\n",
    "    Returns:\n",
    "        (): dataset\n",
    "    \"\"\"\n",
    "    mixer = Mix(img_size=img_size[0])\n",
    "    train_ds, val_ds = create_datasets(train_paths, train_labels, val_paths, val_labels, img_size)\n",
    "    \n",
    "\n",
    "    # train_ds_cm = (\n",
    "    #     train_ds.shuffle(BUFFER_SIZE)\n",
    "    #     .map(lambda x, y: mixer.cutmix(x,y) if tf.random.uniform(()) < 0.5 else x, num_parallel_calls=AUTO)\n",
    "    #     .batch(BATCH_SIZE, num_parallel_calls=AUTO)\n",
    "    #     .prefetch(AUTO)\n",
    "    # )\n",
    "\n",
    "\n",
    "    options = tf_data.Options()\n",
    "    options.autotune.enabled = True\n",
    "    options.autotune.ram_budget = ram_budget\n",
    "    train_ds = train_ds.with_options(options)\n",
    "    \n",
    "    return train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to TFRecord\n",
    "\n",
    "Here we save our processed data into Tensorflow Records so we have a consistent source of training data. For ease we provide the augmented data [here.](https://drive.google.com/file/d/16KvdZW_1Rn5zdopQtbNfej2vXxkhn1i0/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 20:57:05.606170: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-03 20:57:22.977548: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-03 20:59:11.437001: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"resnet\": (224,224),\n",
    "    \"efficientnet\": (380,380),\n",
    "    \"swin_transformer\": (256,256)\n",
    "}\n",
    "if not os.path.exists(\"./records\"):\n",
    "    os.makedirs(\"./records\")\n",
    "for model in models:\n",
    "    model_train_ds, val_ds = create_model_dataset(models[model], models[model][0]*models[model][1]*models[model][1]*BATCH_SIZE)\n",
    "    save_to_tfrecord(model_train_ds, f\"records/{model}_train_no_aug.tfrecord\")\n",
    "    save_to_tfrecord(val_ds, f\"records/{model}_val_no_aug.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Test Data to Record\n",
    "\n",
    "Here we resize the testing data to evaluate our trained and optimized models later. We also provide this here [ResNet](https://drive.google.com/file/d/1FD4bQNdrjlFbQ5hj0PW3gE34FTNYpDaQ/view?usp=sharing) [Swin](https://drive.google.com/file/d/1f-L6LkVVF34c8h0VdIEXsX8wV5H-UK7B/view?usp=sharing) [EffNet](https://drive.google.com/file/d/1rF2CbnYUJctaalbUcUgUVyTtO5yTutk_/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 19:33:00.146539: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.226395: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.226453: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228405: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228510: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228529: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228873: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228913: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-03-28 19:33:02.228941: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:906] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.3/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-28 19:33:02.228955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21241 MB memory:  -> device: 0, name: AMD Radeon RX 7900 XTX, pci bus id: 0000:00:00.3\n",
      "2025-03-28 19:33:34.336859: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-03-28 19:33:54.193467: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"resnet\": (224,224),\n",
    "    \"efficientnet\": (380,380),\n",
    "    \"swin_transformer\": (256,256)\n",
    "}\n",
    "test_paths = test_df[\"id\"].array\n",
    "for model in models:\n",
    "    preprocess = Preprocess(RESIZE_SIZE, models[model])\n",
    "    # Shuffles and batches the datasets\n",
    "    test_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "        .map(lambda filename: (preprocess.resize_augment_image(PATH+filename, augment=False, c_jitter=False)), num_parallel_calls=AUTO, deterministic=True)\n",
    "        .batch(BATCH_SIZE, num_parallel_calls=AUTO, deterministic=True)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    save_test_tfrecord(test_ds, f\"records/{model}_test.tfrecord\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12stad68",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
